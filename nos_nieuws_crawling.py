import streamlit as st
from datetime import datetime
from collections import Counter
import csv
import re
import io
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET

st.set_page_config(page_title="NOS RSS crawler", layout="wide")
st.title("NOS RSS crawler & trefwoorden")

# --------------------- Disclaimer ---------------------
st.info(
    """
    ì´ ì•±ì€ **NOSì˜ ê³µì‹ RSS í”¼ë“œ**ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë³¸ë¬¸ ì „ë¬¸ì„ ì €ì¥/ë°°í¬í•˜ì§€ ì•Šìœ¼ë©°,
    í™”ë©´ì—ëŠ” **ì§§ì€ ìš”ì•½(ìµœëŒ€ 3ë¬¸ì¥Â·500ì)** ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. ëª¨ë“  ì €ì‘ê¶Œì€ ì› ì¶œì²˜(NOS.nl)ì— ìˆìŠµë‹ˆë‹¤.
    """,
    icon="â„¹ï¸",
)

# --------------------- Stopwoorden ---------------------
dutch_stopwords = {
    "de", "en", "van", "ik", "te", "dat", "die", "in", "een", "hij", "het", "niet",
    "zijn", "is", "was", "op", "aan", "met", "als", "voor", "had", "er", "maar",
    "om", "hem", "dan", "zou", "of", "wat", "mijn", "men", "dit", "zo", "door",
    "over", "ze", "zich", "bij", "ook", "tot", "je", "mij", "uit", "der", "daar",
    "haar", "naar", "heb", "hoe", "heeft", "hebben", "deze", "u", "want", "nog",
    "zal", "me", "zij", "nu", "ge", "geen", "omdat", "iets", "worden", "toch",
    "al", "waren", "veel", "meer", "doen", "toen", "ëª¨et", "ben", "zonder", "kan",
    "hun", "dus", "alles", "onder", "ja", "werd", "wezen", "zelf", "tegen",
    "komen", "goed", "hier", "wie", "waarom"
}

# --------------------- Helper: safe preview ---------------------
def make_safe_preview(md_text: str, max_sentences: int = 3, max_chars: int = 500) -> str:
    if not md_text:
        return ""
    text = re.sub(r"\s+", " ", md_text).strip()
    sentences = re.split(r"(?<=[.!?])\s+", text)
    preview = " ".join(sentences[:max_sentences]).strip()
    if len(preview) > max_chars:
        cut = preview[:max_chars]
        if " " in cut:
            cut = cut.rsplit(" ", 1)[0]
        preview = cut.rstrip(" .,;:") + "â€¦"
    return preview

# --------------------- Trefwoordenextractie ---------------------
def extract_keywords(text: str, top_n: int = 10):
    words = re.findall(r"\b[a-zA-Z]{8,}\b", text)
    capitalized_words = [w.capitalize() for w in words]
    filtered_words = [w for w in capitalized_words if w.lower() not in dutch_stopwords]
    freq = Counter(filtered_words)

    unique_keywords = []
    for word, _ in freq.most_common():
        if word not in unique_keywords:
            unique_keywords.append(word)
        if len(unique_keywords) >= top_n:
            break

    return [(kw, freq[kw]) for kw in unique_keywords], filtered_words

# --------------------- HTML to text ---------------------
def html_to_text(html: str) -> str:
    soup = BeautifulSoup(html or "", "html.parser")
    return soup.get_text(separator=" ", strip=True)

# --------------------- RSS Fetch (í‘œì¤€ XMLë¡œ) ---------------------
def fetch_rss_items(feed_url: str, count: int = 5, progress_bar=None):
    try:
        resp = requests.get(feed_url, timeout=10)
        resp.raise_for_status()
    except Exception as e:
        st.error(f"RSS í”¼ë“œ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

    # RSS í”¼ë“œ íŒŒì‹± (ElementTree)
    try:
        tree = ET.fromstring(resp.content)
    except Exception as e:
        st.error(f"RSS XML íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

    # RSS 2.0: <item>ë“¤, Atomì¼ ê²½ìš° <entry>ë“¤
    items = tree.findall(".//item")
    if not items:
        # Atom ì§€ì›
        items = tree.findall(".//{http://www.w3.org/2005/Atom}entry")
        atom = True
    else:
        atom = False

    total = min(count, len(items))
    out = []

    for i, item in enumerate(items[:total], start=1):
        if atom:
            get = lambda tag: item.find(f"{{http://www.w3.org/2005/Atom}}{tag}")
            title = get("title").text if get("title") is not None else "(geen titel)"
            link = None
            for l in item.findall(f"{{http://www.w3.org/2005/Atom}}link"):
                if l.attrib.get("type") == "text/html" or l.attrib.get("rel") == "alternate":
                    link = l.attrib.get("href")
                    break
            link = link or (get("link").attrib.get("href") if get("link") is not None else "")
            summary_html = get("summary").text if get("summary") is not None else ""
        else:
            get = lambda tag: item.find(tag)
            title = get("title").text if get("title") is not None else "(geen titel)"
            link = get("link").text if get("link") is not None else ""
            summary_html = get("description").text if get("description") is not None else ""

        summary_txt = html_to_text(summary_html)
        keywords, _ = extract_keywords(f"{title}\n{summary_txt}")

        out.append({
            "title": title,
            "url": link,
            "summary": summary_txt,
            "keywords": [kw for kw, _ in keywords],
        })

        if progress_bar:
            progress_bar.progress(int(i / max(total, 1) * 100))

    if progress_bar:
        progress_bar.empty()

    return out

# --------------------- CSV ---------------------
def generate_csv_bytes(result):
    if not result:
        return None
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=["index", "title", "url", "keywords"])
    writer.writeheader()
    for idx, item in enumerate(result, 1):
        writer.writerow({
            "index": idx,
            "title": item["title"],
            "url": item["url"],
            "keywords": ", ".join(item["keywords"]),
        })
    return output.getvalue().encode("utf-8-sig")

# --------------------- UI ---------------------
presets = {
    "NOS Nieuws Algemeen": "https://feeds.nos.nl/nosnieuwsalgemeen",
    "NOS Binnenland": "https://feeds.nos.nl/nosnieuwsbinnenland",
    "NOS Buitenland": "https://feeds.nos.nl/nosnieuwsbuitenland",
    "NOS Politiek": "https://feeds.nos.nl/nosnieuwspolitiek",
    "NOS Economie": "https://feeds.nos.nl/nosnieuwseconomie",
    "NOS Tech": "https://feeds.nos.nl/nosnieuwstech",
    "NOS Opmerkelijk": "https://feeds.nos.nl/nosnieuwsopmerkelijk",
    "NOS Sport Algemeen": "https://feeds.nos.nl/nossportalgemeen",
}

left, right = st.columns([2, 3])
with left:
    preset_name = st.selectbox("RSS í”„ë¦¬ì…‹ ì„ íƒ", list(presets.keys()) + ["Custom"])

with right:
    default_url = presets.get(preset_name, "https://feeds.nos.nl/nosnieuwsalgemeen")
    feed_url = st.text_input("ë˜ëŠ” RSS URL ì§ì ‘ ì…ë ¥", value=default_url, help="ë§¤ì²´ì˜ ê³µì‹ RSS URLì„ ì…ë ¥í•˜ì„¸ìš”.")

article_count = st.slider("í•­ëª© ìˆ˜", 1, 20, 5)

# --------------------- Start Crawling ---------------------
if st.button("RSS ê°€ì ¸ì˜¤ê¸°"):
    progress = st.progress(0)
    result = fetch_rss_items(feed_url, article_count, progress)
    if result:
        st.session_state["rss_result"] = result
        st.success(f"âœ… {len(result)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

# --------------------- Show Results ---------------------
if "rss_result" in st.session_state:
    result = st.session_state["rss_result"]

    for i, item in enumerate(result, 1):
        st.markdown(f"### {i}. {item['title']}")
        st.markdown(f"ğŸ”— [ì›ë¬¸ ë§í¬]({item['url']})")
        st.markdown(f"**Trefwoorden:** {', '.join(item['keywords'])}")
        with st.expander("ìš”ì•½ ë³´ê¸° (ìµœëŒ€ 3ë¬¸ì¥Â·500ì)"):
            preview = make_safe_preview(item["summary"])
            st.markdown(preview or "(ìš”ì•½ ì—†ìŒ)")
            if item["url"]:
                st.markdown(f"_Bron: [NOS.nl]({item['url']})_")

    if st.button("ğŸ“¥ CSV ì €ì¥í•˜ê¸°"):
        csv_bytes = generate_csv_bytes(result)
        if csv_bytes:
            st.session_state["csv_data"] = csv_bytes
            st.success("âœ… CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if "csv_data" in st.session_state:
        st.download_button(
            label="ğŸ“„ CSV ë‹¤ìš´ë¡œë“œ",
            data=st.session_state["csv_data"],
            file_name=f"nos_rss_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv",
        )
